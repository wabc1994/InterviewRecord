# 腾讯面了不到20分钟

1. 介绍下项目？



# TCP长连接保活机制,服务端为何需要保活机制心跳机制

**概念**

在使用TCP长连接（复用已建立TCP连接）的场景下，需要对TCP连接进行保活，避免被网关干掉连接。


2. 服务器是如何知道长短连接的？

心跳，就是在TCP长连接中，客户端和服务器之间定期发送一种特殊的数据包，通知对方



如果没有特意的设置某些选项或者实现应用层心跳包，TCP空闲的时候是不会发送任何数据包。也就是说，当一个TCP的socket，客户端与服务端谁也不发送数据，会一直保持着连接。
这其中如果有一方异常掉线（例如死机、路由被破坏、防火墙切断连接等），
另一端如果没有发送数据，永远也不可能知道。这对于一些服务型的程序来说，是灾难性的后果，将会导致服务端socket资源耗尽。


所以为了保证连接的有效性、及时有效地检测到一方的非正常断开，保证连接的资源被有效的利用，我们就会需要一种保活的机制，通常改机制两种处理方式：
        
1. 利用TCP协议层实现的Keepalive；
2. 自己在应用层实现心跳包。


**why**

因为网络是不可靠的，有可能再TCP保持连接的过程当中，由于某些突发情况，例如网线被拔掉了，突然掉电等，会造成服务器和客户单的连接中断，

在这些突发情况下, 如果恰好服务器和客户端之间没有交互的话, 那么它们是不能在短时间内发现对方已经掉线的. 
为了解决这个问题, 我们就需要引入 心跳 机制. 心跳机制的工作原理是: 在服务器和客户端之间一定时间内没有数据交互时, 即处于 idle 状态时, 客户端或服务器会发送一个特殊的数据包给对方, 当接收方收到这个数据报文后, 也立即发送一个特殊的数据报文, 回应发送方, 此即一个 PING-PONG 交互. 自然地, 当某一端收到心跳消息后, 就知道了对方仍然在线, 这就确保 TCP 连接的有效性

**长连接环境为什么要保活**

**how**

我们可以通过两种方式是实现心跳机制：

- 使用TCP协议层面的keepalive 机制

- 在应用层上实现自定义的心跳机制


## 内核系统层次的参数设置

### 设置

而Linux已提供的TCP KEEPALIVE，在应用层可不关心心跳包何时发送、发送什么内容，由OS管理：OS会在该TCP连接上定时发送探测包，探测包既起到连接保活的作用，也能自动检测连接的有效性，并自动关闭无效连接。


**Linux设置**


```java

# cat /proc/sys/net/ipv4/tcp_keepalive_time
7200
# cat /proc/sys/net/ipv4/tcp_keepalive_intvl
75
# cat /proc/sys/net/ipv4/tcp_keepalive_probes
9

```

上面这三个东西是在tcp当中的SO_KEEPALIVE参数进行设置的

关于该参数的设置

![](https://github.com/wabc1994/InterviewRecord/blob/master/InterviewHistory/pic/SO_KEEPALIVE%E5%8F%82%E6%95%B0.png)



- tcp_keepalive_time，在TCP保活打开的情况下，如果在该时间内没有数据往来，则发送探测包。即允许的持续空闲时长，或者说每次正常发送心跳的周期，默认值为7200s（2h）。
- tcp_keepalive_probes 尝试探测的次数。如果发送的探测包次数超过该值仍然没有收到对方响应，则认为连接已失效并关闭连接。默认值为9（次）
- tcp_keepalive_intvl，探测包发送间隔时间。默认值为75s。


**具体流程情况**
- KeepAlive 机制开启后，在一定时间内（一般时间为 7200s，参数 tcp_keepalive_time）在链路上没有数据传送的情况下，TCP 层将发送相应的 KeepAlive 探针以确定连接可用性，
- 探测失败后重试 9（参数 tcp_keepalive_probes）次，每次间隔时间 75s（参数 tcp_keepalive_intvl），所有探测失败后，才认为当前连接已经不可用。

### tcp keepalive 缺陷


KeepAlive 机制是在网络层面保证了连接的可用性，但站在应用框架层面我们认为这还不够。主要体现在三个方面：

虽然在TCP协议层上面，提供了keepalive 保活机制，但是使用它有几个缺点：

- 它不是TCP的标准协议，并且是默认关闭的

- TCP keepalive 机制依赖于操作系统的实现, 默认的 keepalive 心跳时间是 两个小时, 并且对 keepalive 的修改需要系统调用(或者修改系统配置), 灵活性不够.

- KeepAlive 本身是面向网络的，并不面向于应用，当连接不可用，可能是由于应用本身的 GC 频繁，系统 load 高等情况，但网络仍然是通的，此时，应用已经失去了活性，连接应该被认为是不可用的。



### 总结

有三种使用 KeepAlive 的实践方案：

1. 默认情况下使用 KeepAlive 周期为 2 个小时，如不选择更改，属于误用范畴，造成资源浪费：内核会为每一个连接都打开一个保活计时器，N 个连接会打开 N 个保活计时器。 优势很明显：
    - TCP 协议层面保活探测机制，系统内核完全替上层应用自动给做好了
    - 内核层面计时器相比上层应用，更为高效
    - 层应用只需要处理数据收发、连接异常通知即可

2. 关闭 TCP 的 KeepAlive，完全使用应用层心跳保活机制。由应用掌管心跳，更灵活可控，比如可以在应用级别设置心跳周期，适配私有协议。

3. 业务心跳 + TCP KeepAlive 一起使用，互相作为补充，但 TCP 保活探测周期和应用的心跳周期要协调，以互补方可，不能够差距过大，否则将达不到设想的效果。

[使用TCP keepalive还是HeartBeat心跳包？](https://www.cnblogs.com/zhuyf87/archive/2013/01/28/2880272.html)

[来源连接](https://blog.csdn.net/lu930124/article/details/77692997)

关于心跳机制

## 应用层实现

1. 心跳检测
2. 定时器


3. 为何重发三个ack认为网络发生了拥塞？
> 短连接和长连接的优势，分别是对方的，想要图简单，


4. tcp 是如何保证可靠的传输情况的 

   [TCP如何保证传输可靠性 - 每天进步一点点！ - ITeye博客](https://uule.iteye.com/blog/2429131)
   
   - 校验和 在这里面是有可能再来使用的情况
   - 确认应答 + 序列号
   - 超时重传
   - 流量控制
   - 拥塞控制
   
5. 注意HTTP的keepAlive 和tcp层的KeepAlive不是同一个东西，在该种情况下面

- HTTP 协议的 KeepAlive 意图在于连接复用，同一个连接上串行方式传递请求-响应数据
- 

[TCP keepalive 和 http keep-alive](https://segmentfault.com/a/1190000012894416)